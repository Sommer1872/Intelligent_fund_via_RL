{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#sns.set_style(\"whitegrid\")\n",
    "#current_palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from thurner_model import NoiseTrader, Fund, DynamicFund, find_equilibrium \n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    \"\"\"\n",
    "    Docstring\n",
    "    \"\"\"\n",
    "    # Define our fundamental value V and asset-supply N\n",
    "    V = 1.\n",
    "    N = 1000.\n",
    "    \n",
    "    initial_price = 1.\n",
    "    \n",
    "    # Define noise trader (uses the NoiseTrader Class defined in thurner_model.py)\n",
    "    roh_nt = 0.99\n",
    "    sigma_nt = 0.035\n",
    "    noise_trader = NoiseTrader(roh_nt, sigma_nt, V, N)\n",
    "    initial_nt_spending = V*N\n",
    "    \n",
    "    def __init__(self):\n",
    "        # tracks trader spending\n",
    "        \n",
    "        self.p_t = self.initial_price\n",
    "        self.xi_t = self.initial_nt_spending\n",
    "        self.done = False \n",
    "        \n",
    "    # when resetting the environment, we set the state back to the initial one \n",
    "    def reset(self):\n",
    "        self.p_t = self.initial_price\n",
    "        self.xi_t = self.initial_nt_spending\n",
    "\n",
    "    def step(self, funds):\n",
    "        \"\"\"Finds equilibrium, and updates environment parameters\"\"\" \n",
    "        # track the old price for the investor mechanism\n",
    "        p_tm1 = self.p_t\n",
    "        \n",
    "        # 1. Find the new price for the timestep\n",
    "        self.xi_t = self.noise_trader.cash_spending(self.xi_t)\n",
    "        self.p_t = find_equilibrium(self.xi_t, funds)\n",
    "    \n",
    "        # 2. update the holdings of all the funds (wealth, shares and cash)\n",
    "        current_wealth = []\n",
    "        \n",
    "        for fund in funds:\n",
    "            fund.update_holdings(self.p_t)\n",
    "            fund.check_and_make_bankrupt(self.p_t)\n",
    "            \n",
    "            fund.process_inflows(p_tm1, self.p_t)\n",
    "            \n",
    "            new_wealth_of_fund = fund.get_wealth(self.p_t)\n",
    "            current_wealth.append(new_wealth_of_fund)\n",
    "            \n",
    "            # set done to True if one fund increases its wealth 50-fold\n",
    "            if new_wealth_of_fund > 50*fund.initial_wealth:\n",
    "                self.done = True\n",
    "                \n",
    "        return current_wealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 10, 15, 20, 25, 30, 35, 40, 45, 25]\n"
     ]
    }
   ],
   "source": [
    "# Create examples for observations to train the featurizer\n",
    "tracked_fund = DynamicFund(25)\n",
    "other_funds = [DynamicFund((i+1)*5) for i in range(9)]\n",
    "other_funds.append(tracked_fund)\n",
    "print([f.beta for f in other_funds])\n",
    "\n",
    "states = np.zeros((10000,2))\n",
    "for i in range(10000):\n",
    "    current_wealth = env.step(other_funds)\n",
    "    states[i] = np.array([env.p_t,\n",
    "                         tracked_fund.get_wealth(env.p_t)])\n",
    "    # record the state of the fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
      "       transformer_weights=None)\n"
     ]
    }
   ],
   "source": [
    "# (By Denny Britz)\n",
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "\n",
    "observation_examples = states \n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to converte a state to a featurized represenation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "print(featurizer.fit(scaler.transform(observation_examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_state(state):\n",
    "    \"\"\"\n",
    "    Returns the featurized representation for a state.\n",
    "    \"\"\"\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LearningFund(DynamicFund):\n",
    "    \"\"\"\n",
    "    Description: Base Class is DynamicFund, defined in thurner_model.py\n",
    "    The Learning Fund learns its demand function via reinforcement learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The learning fund does not need a beta\n",
    "        \n",
    "        self.cash = self.initial_wealth\n",
    "        self.shares = 0 \n",
    "        self.activation_delay = 0\n",
    "        \n",
    "        self.performance = 0.0\n",
    "        self.previous_wealth = self.initial_wealth\n",
    "        self.previous_investment = 0.0\n",
    "   \n",
    "    def get_state(self, p_t):\n",
    "        # The state is composed of the current price, as well as the current\n",
    "        # holdings of the fund\n",
    "        state = np.array([p_t,\n",
    "                          self.get_wealth(p_t)])\n",
    "        return state\n",
    "\n",
    "    def get_demand(self, p_t): # this needs to be a function of p_t for the market clearing to work\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p_t : current_price in the environment\n",
    "        Returns:\n",
    "            A number for the learning_fund's demand, estimated by the\n",
    "            policy_estimator, based on the current state\n",
    "        \"\"\" \n",
    "        state = self.get_state(p_t)\n",
    "        \n",
    "        return policy_estimator.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. Also called Actor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just linear classifier\n",
    "            self.mu = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            \n",
    "            self.mu = tf.squeeze(self.mu)\n",
    "            \n",
    "            self.sigma = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            \n",
    "            self.sigma = tf.squeeze(self.sigma)\n",
    "            self.sigma = tf.nn.softplus(self.sigma) + 1e-5\n",
    "            self.normal_dist = tf.contrib.distributions.Normal(self.mu,\n",
    "                                                               self.sigma)\n",
    "            self.demand = self.normal_dist._sample_n(1)\n",
    "            \n",
    "            # clip the demand, maximum demand is given by:\n",
    "            # (see Thurner et al. 2012 for formula)\n",
    "            max_demand = learning_fund.lambda_max * \\\n",
    "                            learning_fund.get_wealth(env.p_t) / env.p_t\n",
    "                \n",
    "            self.demand = tf.clip_by_value(self.demand, 0, max_demand)\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -self.normal_dist.log_prob(self.demand) * self.target\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.loss -= 1e-1 * self.normal_dist.entropy()\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.demand, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, demand, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state,\n",
    "                      self.target: target,\n",
    "                      self.demand: demand  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. Also called Critic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just a linear classifier\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actor_critic(env, policy_estimator, value_estimator, num_episodes, num_timesteps=5000, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy \n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: My self created environment, specified above.\n",
    "        policy_estimator: Policy Function to be optimized \n",
    "        value_estimator: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and\n",
    "        episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\",\n",
    "                                                       \"reward\", \"next_state\",\n",
    "                                                       \"done\"])\n",
    "    \n",
    "    funds_wealth_all_episodes = []\n",
    "    \n",
    "    learning_fund_stats = np.zeros((num_episodes, num_timesteps, 5))\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # Reset everything\n",
    "        prices = []\n",
    "        funds_wealth = []\n",
    "\n",
    "        # Create our learning_fund\n",
    "        learning_fund = LearningFund()\n",
    "        \n",
    "        # Create the funds \n",
    "        number_of_funds = 10\n",
    "        funds = [DynamicFund((i+1)*5) for i in range(number_of_funds)]\n",
    "        \n",
    "        # Add our learning fund\n",
    "        funds.append(learning_fund)\n",
    "\n",
    "        # Reset the environment \n",
    "        env.reset() \n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in range(num_timesteps):\n",
    "            \n",
    "            # get the demand (which is our action) of the learning fund\n",
    "            demand = learning_fund.get_demand(env.p_t) \n",
    "            \n",
    "            state = learning_fund.get_state(env.p_t)\n",
    "            \n",
    "            # Simulate a step in the environment,\n",
    "            # record the wealth of all funds in current_wealth\n",
    "            current_wealth = env.step(funds)\n",
    "            \n",
    "            # record the wealth of all funds and the current price\n",
    "            funds_wealth.append(current_wealth)\n",
    "            prices.append(env.p_t)\n",
    "            \n",
    "            # we assume one learning fund for the moment\n",
    "            next_state = np.array([env.p_t,\n",
    "                                   learning_fund.get_wealth(env.p_t)]) \n",
    "\n",
    "            reward = learning_fund.performance\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(state=state, action=demand,\n",
    "                                      reward=reward, next_state=next_state,\n",
    "                                      done=env.done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = value_estimator.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - value_estimator.predict(state)\n",
    "            \n",
    "            # Update the value estimator\n",
    "            value_estimator.update(state, td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            policy_estimator.update(state, td_error, demand)\n",
    "            \n",
    "            learning_fund_stats[i_episode][t] = np.array([env.p_t,\n",
    "                                                          demand,\n",
    "                                                          learning_fund.get_wealth(env.p_t),\n",
    "                                                          learning_fund.cash,\n",
    "                                                          learning_fund.shares])\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rt: {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes,\n",
    "                    stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "            \n",
    "            #if t%500 == 0:\n",
    "            #    print(\"P_t: \", env.p_t)\n",
    "            #    print(\"Demand: \", demand)\n",
    "            #    print(\"Shares:\", learning_fund.shares)\n",
    "            #    print(\"Reward:\", learning_fund.performance, \"\\n\")\n",
    "            #    print(funds[5].get_demand(env.p_t))\n",
    "            #    print(funds[5].shares)\n",
    "            #    print(funds[5].performance)\n",
    "            #    print(\"\")\n",
    "                        \n",
    "            # env.done is True if one fund increases its wealth 50-fold\n",
    "            if env.done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "        # After each episode, record the wealth of all funds\n",
    "        funds_wealth_all_episodes.append(funds_wealth)\n",
    "    \n",
    "        # Save the variables to disk.\n",
    "        \n",
    "        checkpoint = \"./checkpoints/{}-ep{}\".format(experiment_name,i_episode)\n",
    "        save_path = saver.save(sess,checkpoint)         \n",
    "        print(\"\\nModel saved in path: {}\\n\".format(save_path))\n",
    "    \n",
    "    return stats, funds_wealth_all_episodes, learning_fund_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/more_episodes_5000ts_30e-ep29\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value policy_estimator/fully_connected/weights\n\t [[Node: policy_estimator/fully_connected/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@policy_estimator/fully_connected/weights\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](policy_estimator/fully_connected/weights)]]\n\nCaused by op 'policy_estimator/fully_connected/weights/read', defined at:\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-3be21ec1c0e3>\", line 5, in <module>\n    policy_estimator = PolicyEstimator(learning_rate=0.001)\n  File \"<ipython-input-9-880b450768f8>\", line 16, in __init__\n    weights_initializer=tf.zeros_initializer)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1661, in fully_connected\n    outputs = layer.apply(inputs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 443, in __call__\n    self.build(input_shapes[0])\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 118, in build\n    trainable=True)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 383, in add_variable\n    trainable=trainable and self.trainable)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1561, in layer_variable_getter\n    return _model_variable_getter(getter, *args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1553, in _model_variable_getter\n    custom_getter=getter, use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 261, in model_variable\n    use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 216, in variable\n    use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 330, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1400, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value policy_estimator/fully_connected/weights\n\t [[Node: policy_estimator/fully_connected/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@policy_estimator/fully_connected/weights\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](policy_estimator/fully_connected/weights)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value policy_estimator/fully_connected/weights\n\t [[Node: policy_estimator/fully_connected/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@policy_estimator/fully_connected/weights\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](policy_estimator/fully_connected/weights)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-3be21ec1c0e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     stats,funds_wealth_all_episodes,learnin_fund_stats = actor_critic(env, policy_estimator,\n\u001b[1;32m     21\u001b[0m                                                                       \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                                                                       num_timesteps=timesteps, discount_factor=0.95) \n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9aa34e242def>\u001b[0m in \u001b[0;36mactor_critic\u001b[0;34m(env, policy_estimator, value_estimator, num_episodes, num_timesteps, discount_factor)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;31m# get the demand (which is our action) of the learning fund\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mdemand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_fund\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_demand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_fund\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0ae3d13e1cc3>\u001b[0m in \u001b[0;36mget_demand\u001b[0;34m(self, p_t)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-880b450768f8>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state, sess)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdemand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdemand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value policy_estimator/fully_connected/weights\n\t [[Node: policy_estimator/fully_connected/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@policy_estimator/fully_connected/weights\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](policy_estimator/fully_connected/weights)]]\n\nCaused by op 'policy_estimator/fully_connected/weights/read', defined at:\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-30-3be21ec1c0e3>\", line 5, in <module>\n    policy_estimator = PolicyEstimator(learning_rate=0.001)\n  File \"<ipython-input-9-880b450768f8>\", line 16, in __init__\n    weights_initializer=tf.zeros_initializer)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1661, in fully_connected\n    outputs = layer.apply(inputs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 503, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 443, in __call__\n    self.build(input_shapes[0])\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/core.py\", line 118, in build\n    trainable=True)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/layers/base.py\", line 383, in add_variable\n    trainable=trainable and self.trainable)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 1065, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 962, in get_variable\n    use_resource=use_resource, custom_getter=custom_getter)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 360, in get_variable\n    validate_shape=validate_shape, use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1561, in layer_variable_getter\n    return _model_variable_getter(getter, *args, **kwargs)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/layers.py\", line 1553, in _model_variable_getter\n    custom_getter=getter, use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 261, in model_variable\n    use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 216, in variable\n    use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 352, in _true_getter\n    use_resource=use_resource)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\", line 725, in _get_single_variable\n    validate_shape=validate_shape)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 199, in __init__\n    expected_shape=expected_shape)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\", line 330, in _init_from_args\n    self._snapshot = array_ops.identity(self._variable, name=\"read\")\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 1400, in identity\n    result = _op_def_lib.apply_op(\"Identity\", input=input, name=name)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2630, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1204, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value policy_estimator/fully_connected/weights\n\t [[Node: policy_estimator/fully_connected/weights/read = Identity[T=DT_FLOAT, _class=[\"loc:@policy_estimator/fully_connected/weights\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](policy_estimator/fully_connected/weights)]]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "init_op = tf.global_variables_initializer()\n",
    "p_t = .5\n",
    "\n",
    "policy_estimator = PolicyEstimator(learning_rate=0.001)\n",
    "value_estimator = ValueEstimator(learning_rate=0.1)\n",
    "\n",
    "episodes = 2\n",
    "timesteps = 2000\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    latest_checkpoint = tf.train.latest_checkpoint(\"./checkpoints/\")\n",
    "    meta_path = '{}.meta'.format(latest_checkpoint)\n",
    "\n",
    "    saver = tf.train.import_meta_graph(meta_path)\n",
    "    saver.restore(session, latest_checkpoint)\n",
    "    learning_fund = LearningFund()\n",
    "\n",
    "    stats,funds_wealth_all_episodes,learnin_fund_stats = actor_critic(env, policy_estimator,\n",
    "                                                                      value_estimator,num_episodes=episodes,\n",
    "                                                                      num_timesteps=timesteps, discount_factor=0.95) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fund = LearningFund(30)\n",
    "\n",
    "days = []\n",
    "prices = []\n",
    "wealth = []\n",
    "investment = []\n",
    "for i in range(0,120):\n",
    "    price = 1.4 - i/100\n",
    "    fund.update_holdings(price)\n",
    "    days.append(i)\n",
    "    prices.append(price)\n",
    "    wealth.append(fund.get_wealth(price))\n",
    "    investment.append(fund.shares * price)\n",
    "    \n",
    "plt.plot(days, prices, 'b', label='price')\n",
    "plt.plot(days, investment, 'g', label='investment')\n",
    "plt.plot(days, wealth, 'r', label='wealth')\n",
    "plt.xlabel('day')\n",
    "plt.ylabel('amount')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

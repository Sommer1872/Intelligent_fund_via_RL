{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I model the fund as a learning agent, which is able to adjust its demand function over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine the whole reinforcement mechanism only as the demand function of the LearningFund."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "current_palette = sns.color_palette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from thurner_model import NoiseTrader, Fund, DynamicFund, find_equilibrium \n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    \"\"\"\n",
    "    Docstring\n",
    "    \"\"\"\n",
    "    # Define our fundamental value V and asset-supply N\n",
    "    V = 1.\n",
    "    N = 1000.\n",
    "    \n",
    "    initial_price = 1.\n",
    "    \n",
    "    # Define noise trader (uses the NoiseTrader Class defined in thurner_model.py)\n",
    "    roh_nt = 0.99\n",
    "    sigma_nt = 0.035\n",
    "    noise_trader = NoiseTrader(roh_nt, sigma_nt, V, N)\n",
    "    initial_nt_spending = V*N\n",
    "    \n",
    "    def __init__(self):\n",
    "        # tracks trader spending\n",
    "        \n",
    "        self.p_t = self.initial_price\n",
    "        self.xi_t = self.initial_nt_spending\n",
    "        self.done = False \n",
    "        \n",
    "    # when resetting the environment, we set the state back to the initial one \n",
    "    def reset(self):\n",
    "        self.p_t = self.initial_price\n",
    "        self.xi_t = self.initial_nt_spending\n",
    "\n",
    "    def step(self, funds):\n",
    "        \"\"\"Finds equilibrium, and updates environment parameters\"\"\" \n",
    "        # track the old price for the investor mechanism\n",
    "        p_tm1 = self.p_t\n",
    "        \n",
    "        # 1. Find the new price for the timestep\n",
    "        self.xi_t = self.noise_trader.cash_spending(self.xi_t)\n",
    "        self.p_t = find_equilibrium(self.xi_t, funds)\n",
    "    \n",
    "        # 2. update the holdings of all the funds (wealth, shares and cash)\n",
    "        current_wealth = []\n",
    "        \n",
    "        for fund in funds:\n",
    "            fund.update_holdings(self.p_t)\n",
    "            fund.check_and_make_bankrupt(self.p_t)\n",
    "            \n",
    "            fund.process_inflows(p_tm1, self.p_t)\n",
    "            \n",
    "            new_wealth_of_fund = fund.get_wealth(self.p_t)\n",
    "            current_wealth.append(new_wealth_of_fund)\n",
    "            \n",
    "            # set done to True if one fund increases its wealth 50-fold\n",
    "            if new_wealth_of_fund > 50*fund.initial_wealth:\n",
    "                self.done = True\n",
    "                \n",
    "        return current_wealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 10, 15, 20, 25, 30, 35, 40, 45, 25]\n"
     ]
    }
   ],
   "source": [
    "# Create examples for observations to train the featurizer\n",
    "tracked_fund = DynamicFund(25)\n",
    "other_funds = [DynamicFund((i+1)*5) for i in range(9)]\n",
    "other_funds.append(tracked_fund)\n",
    "print([f.beta for f in other_funds])\n",
    "\n",
    "states = np.zeros((10000,5))\n",
    "for i in range(10000):\n",
    "    current_wealth = env.step(other_funds)\n",
    "    states[i] = np.array([env.p_t,\n",
    "                         tracked_fund.get_wealth(env.p_t),\n",
    "                         tracked_fund.cash,\n",
    "                         tracked_fund.shares,\n",
    "                         tracked_fund.activation_delay])\n",
    "    # record the state of the fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureUnion(n_jobs=1,\n",
       "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
       "       transformer_weights=None)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (By Denny Britz)\n",
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "\n",
    "observation_examples = states \n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to converte a state to a featurized represenation.\n",
    "# We use RBF kernels with different variances to cover different parts of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform(observation_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featurize_state(state):\n",
    "    \"\"\"\n",
    "    Returns the featurized representation for a state.\n",
    "    \"\"\"\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningFund(DynamicFund):\n",
    "    \"\"\"\n",
    "    Description: Base Class is DynamicFund, defined in thurner_model.py\n",
    "    The Learning Fund learns its demand function via reinforcement learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The learning fund does not need a beta\n",
    "        \n",
    "        self.cash = self.initial_wealth\n",
    "        self.shares = 0 \n",
    "        self.activation_delay = 0\n",
    "        \n",
    "        self.performance = 0.0\n",
    "        self.previous_wealth = self.initial_wealth\n",
    "        self.previous_investment = 0.0\n",
    "   \n",
    "    def get_state(self, p_t):\n",
    "        # The state is composed of the current price, as well as the current\n",
    "        # holdings of the fund\n",
    "        state = np.array([p_t,\n",
    "                          self.get_wealth(p_t), \n",
    "                          self.cash,\n",
    "                          self.shares,\n",
    "                          self.activation_delay])\n",
    "        return state\n",
    "\n",
    "    def get_demand(self, p_t): # this needs to be a function of p_t for the market clearing to work\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p_t : current_price in the environment\n",
    "        Returns:\n",
    "            A number for the learning_fund's demand, estimated by the\n",
    "            policy_estimator, based on the current state\n",
    "        \"\"\" \n",
    "        state = self.get_state(p_t)\n",
    "        \n",
    "        return policy_estimator.predict(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. Also called Actor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just linear classifier\n",
    "            self.mu = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            self.mu = tf.squeeze(self.mu)\n",
    "            \n",
    "            self.sigma = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            \n",
    "            self.sigma = tf.squeeze(self.sigma)\n",
    "            self.sigma = tf.nn.softplus(self.sigma) + 1e-5\n",
    "            self.normal_dist = tf.contrib.distributions.Normal(self.mu,\n",
    "                                                               self.sigma)\n",
    "            self.demand = self.normal_dist._sample_n(1)\n",
    "            \n",
    "            # clip the demand, maximum demand is given by:\n",
    "            max_demand = learning_fund.lambda_max * \\\n",
    "                            learning_fund.get_wealth(env.p_t) / env.p_t\n",
    "                \n",
    "            self.demand = tf.clip_by_value(self.demand,\n",
    "                                           0,\n",
    "                                           max_demand)\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -self.normal_dist.log_prob(self.demand) * self.target\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.loss -= 1e-1 * self.normal_dist.entropy()\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.demand, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, demand, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state,\n",
    "                      self.target: target,\n",
    "                      self.demand: demand  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. Also called Critic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just a linear classifier\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.contrib.framework.get_global_step())        \n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def actor_critic(env, policy_estimator, value_estimator, num_episodes,\n",
    "                 discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy \n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: My self created environment, specified above.\n",
    "        policy_estimator: Policy Function to be optimized \n",
    "        value_estimator: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and\n",
    "        episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\",\n",
    "                                                       \"reward\", \"next_state\",\n",
    "                                                       \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # Reset everything\n",
    "        prices = []\n",
    "        funds_wealth = []\n",
    "\n",
    "        # Create the funds \n",
    "        number_of_funds = 10\n",
    "        funds = [Fund((i+1)*5) for i in range(number_of_funds)]\n",
    "        \n",
    "        # Add our learning fund\n",
    "        funds.append(learning_fund)\n",
    "\n",
    "        # Reset the environment \n",
    "        env.reset() \n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # get the demand (which is our action) of the learning fund\n",
    "            demand = learning_fund.get_demand(env.p_t) \n",
    "            \n",
    "            state = learning_fund.get_state(env.p_t)\n",
    "            \n",
    "            # Simulate a step in the environment,\n",
    "            # record the wealth of all funds in current_wealth\n",
    "            current_wealth = env.step(funds)\n",
    "            \n",
    "            # record the wealth of all funds and the current price\n",
    "            funds_wealth.append(current_wealth)\n",
    "            prices.append(env.p_t)\n",
    "            \n",
    "            # we assume one learning fund for the moment\n",
    "            next_state = np.array([env.p_t,\n",
    "                                   learning_fund.get_wealth(env.p_t), \n",
    "                                   learning_fund.cash,\n",
    "                                   learning_fund.shares,\n",
    "                                   learning_fund.activation_delay]) \n",
    "\n",
    "            reward = learning_fund.performance \n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(state=state, action=demand,\n",
    "                                      reward=reward, next_state=next_state,\n",
    "                                      done=env.done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = value_estimator.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - value_estimator.predict(state)\n",
    "            \n",
    "            # Update the value estimator\n",
    "            value_estimator.update(state, td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            policy_estimator.update(state, td_error, demand)\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes,\n",
    "                    stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "            \n",
    "            # env.done is True if one fund increases its wealth 50-fold\n",
    "            if env.done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1196 @ Episode 1/50 (0.0)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-3fffce9ee624>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# happening there.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     stats = actor_critic(env, policy_estimator, value_estimator,\n\u001b[0;32m---> 22\u001b[0;31m                          num_episodes=50, discount_factor=0.95)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-db96591d6732>\u001b[0m in \u001b[0;36mactor_critic\u001b[0;34m(env, policy_estimator, value_estimator, num_episodes, discount_factor)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;31m# Calculate TD Target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mvalue_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0mtd_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdiscount_factor\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0mtd_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtd_target\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-111-4069ab75b4df>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state, sess)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_estimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-108-b1a18d2d7fac>\u001b[0m in \u001b[0;36mfeaturize_state\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfeaturized\u001b[0m \u001b[0mrepresentation\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mfeaturized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeaturized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    641\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy,\n\u001b[1;32m    642\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_shape_repr\u001b[0;34m(shape)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mjoined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;31m# special notation for singleton tuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mjoined\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the environment\n",
    "env = Env()\n",
    "\n",
    "# Create our learning_fund\n",
    "learning_fund = LearningFund()\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# This works\n",
    "tf.reset_default_graph()\n",
    "\n",
    "global_step = tf.train.get_global_step()\n",
    "policy_estimator = PolicyEstimator(learning_rate=0.001)\n",
    "value_estimator = ValueEstimator(learning_rate=0.1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Due to randomness in the policy, the number of episodes you need varies\n",
    "    # TODO: Sometimes the algorithm gets stuck, I'm not sure what exactly is\n",
    "    # happening there.\n",
    "    stats = actor_critic(env, policy_estimator, value_estimator,\n",
    "                         num_episodes=50, discount_factor=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0rc4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from thurner_model import NoiseTrader, Fund, DynamicFund, find_equilibrium \n",
    "import plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('rbf1', RBFSampler(gamma=5.0, n_components=100, random_state=None)), ('rbf2', RBFSampler(gamma=2.0, n_components=100, random_state=None)), ('rbf3', RBFSampler(gamma=1.0, n_components=100, random_state=None)), ('rbf4', RBFSampler(gamma=0.5, n_components=100, random_state=None))],\n",
      "       transformer_weights=None)\n"
     ]
    }
   ],
   "source": [
    "class Env:\n",
    "    \"\"\"\n",
    "    Docstring\n",
    "    \"\"\"\n",
    "    # Define our fundamental value V and asset-supply N\n",
    "    V = 1.\n",
    "    N = 1000.\n",
    "    \n",
    "    initial_price = 1.\n",
    "    \n",
    "    # Define noise trader (NoiseTrader Class defined in thurner_model.py)\n",
    "    roh_nt = 0.99\n",
    "    sigma_nt = 0.035\n",
    "    noise_trader = NoiseTrader(roh_nt, sigma_nt, V, N)\n",
    "    initial_nt_spending = V*N\n",
    "    \n",
    "    def __init__(self):\n",
    "        # tracks trader spending\n",
    "        \n",
    "        self.p_t = self.initial_price\n",
    "        self.xi_t = self.initial_nt_spending\n",
    "        self.done = False \n",
    "        \n",
    "    # when resetting the environment, we set the state back to the initial one \n",
    "    def reset(self):\n",
    "        self.p_t = self.initial_price\n",
    "        self.xi_t = self.initial_nt_spending\n",
    "\n",
    "    def step(self, funds):\n",
    "        \"\"\"Finds equilibrium, and updates environment parameters\"\"\" \n",
    "        # track the old price for the investor mechanism\n",
    "        p_tm1 = self.p_t\n",
    "        \n",
    "        # 1. Find the new price for the timestep\n",
    "        self.xi_t = self.noise_trader.cash_spending(self.xi_t)\n",
    "        self.p_t = find_equilibrium(self.xi_t, funds)\n",
    "    \n",
    "        # 2. update the holdings of all the funds (wealth, shares and cash)\n",
    "        current_wealth = []\n",
    "        \n",
    "        for fund in funds:\n",
    "            fund.update_holdings(self.p_t)\n",
    "            fund.check_and_make_bankrupt(self.p_t)\n",
    "            \n",
    "            fund.process_inflows(p_tm1, self.p_t)\n",
    "            \n",
    "            new_wealth_of_fund = fund.get_wealth(self.p_t)\n",
    "            current_wealth.append(new_wealth_of_fund)\n",
    "            \n",
    "            # set done to True if one fund increases its wealth 50-fold\n",
    "            if new_wealth_of_fund > 50*fund.initial_wealth:\n",
    "                self.done = True\n",
    "                \n",
    "        return current_wealth\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "env = Env()\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "# Create examples for observations to train the featurizer\n",
    "tracked_fund = DynamicFund(25)\n",
    "other_funds = [DynamicFund((i+1)*5) for i in range(9)]\n",
    "other_funds.append(tracked_fund)\n",
    "#print([f.beta for f in other_funds])\n",
    "\n",
    "states = np.zeros((10000,2))\n",
    "for i in range(10000):\n",
    "    current_wealth = env.step(other_funds)\n",
    "    states[i] = np.array([env.p_t,\n",
    "                          tracked_fund.get_wealth(env.p_t)\n",
    "                         ])\n",
    "    # record the state of the fund\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "# (By Denny Britz)\n",
    "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
    "# We use a few samples from the observation space to do this\n",
    "\n",
    "observation_examples = states \n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler.fit(observation_examples)\n",
    "\n",
    "# Used to converte a state to a featurized represenation.\n",
    "# We use RBF kernels with different variances to cover different parts\n",
    "# of the space\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "print(featurizer.fit(scaler.transform(observation_examples)))\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def featurize_state(state):\n",
    "    \"\"\"\n",
    "    Returns the featurized representation for a state.\n",
    "    \"\"\"\n",
    "    scaled = scaler.transform([state])\n",
    "    featurized = featurizer.transform(scaled)\n",
    "    return featurized[0]\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "class LearningFund(DynamicFund):\n",
    "    \"\"\"\n",
    "    Description: Base Class is DynamicFund, defined in thurner_model.py\n",
    "    The Learning Fund learns its demand function via reinforcement learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The learning fund does not need a beta, but learns how to leverage\n",
    "        self.leverage = 0\n",
    "        \n",
    "        self.cash = self.initial_wealth\n",
    "        self.shares = 0 \n",
    "        self.activation_delay = 0\n",
    "        \n",
    "        self.performance = 0.0\n",
    "        self.previous_wealth = self.initial_wealth\n",
    "        self.previous_investment = 0.0\n",
    "   \n",
    "    def get_state(self, p_t):\n",
    "        # The state is composed of the current price, as well as the current\n",
    "        # holdings of the fund\n",
    "        state = np.array([p_t,\n",
    "                          self.get_wealth(p_t)])\n",
    "        return state\n",
    "\n",
    "    def get_demand(self, p_t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p_t : current_price in the environment\n",
    "        Returns:\n",
    "            A number for the learning_fund's demand, estimated by the\n",
    "            policy_estimator, based on the current state\n",
    "        \"\"\" \n",
    "        state = self.get_state(p_t)\n",
    "        \n",
    "        self.leverage = policy_estimator.predict(state)\n",
    "        \n",
    "        return self.leverage * self.get_wealth(p_t) / p_t \n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "class PolicyEstimator():\n",
    "    \"\"\"\n",
    "    Policy Function approximator. Also called Actor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, scope=\"policy_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just linear classifier\n",
    "            self.mu = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            \n",
    "            self.mu = tf.squeeze(self.mu)\n",
    "            \n",
    "            self.sigma = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "            \n",
    "            self.sigma = tf.squeeze(self.sigma)\n",
    "            self.sigma = tf.nn.softplus(self.sigma) + 1e-5\n",
    "            self.normal_dist = tf.contrib.distributions.Normal(self.mu,\n",
    "                                                               self.sigma)\n",
    "            leverage = self.normal_dist._sample_n(1)\n",
    "            \n",
    "            # clip the leverage, maximum leverage is given by:\n",
    "            max_leverage = learning_fund.lambda_max \n",
    "                \n",
    "            self.leverage = tf.clip_by_value(leverage, 0, max_leverage)\n",
    "\n",
    "            # Loss and train op\n",
    "            self.loss = -self.normal_dist.log_prob(self.leverage) * self.target\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.loss -= 1e-1 * self.normal_dist.entropy()\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.leverage, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, leverage, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state,\n",
    "                      self.target: target,\n",
    "                      self.leverage: leverage  }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "class ValueEstimator():\n",
    "    \"\"\"\n",
    "    Value Function approximator. Also called Critic.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1, scope=\"value_estimator\"):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [400], \"state\")\n",
    "            self.target = tf.placeholder(dtype=tf.float32, name=\"target\")\n",
    "\n",
    "            # This is just a linear classifier\n",
    "            self.output_layer = tf.contrib.layers.fully_connected(\n",
    "                inputs=tf.expand_dims(self.state, 0),\n",
    "                num_outputs=1,\n",
    "                activation_fn=None,\n",
    "                weights_initializer=tf.zeros_initializer)\n",
    "\n",
    "            self.value_estimate = tf.squeeze(self.output_layer)\n",
    "            self.loss = tf.squared_difference(self.value_estimate, self.target)\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.train_op = self.optimizer.minimize(\n",
    "                self.loss, global_step=tf.train.get_global_step())\n",
    "    \n",
    "    def predict(self, state, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        return sess.run(self.value_estimate, { self.state: state })\n",
    "\n",
    "    def update(self, state, target, sess=None):\n",
    "        sess = sess or tf.get_default_session()\n",
    "        state = featurize_state(state)\n",
    "        feed_dict = { self.state: state, self.target: target }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict)\n",
    "        return loss\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "def actor_critic(env, policy_estimator, value_estimator, num_episodes, num_timesteps=5000, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Actor Critic Algorithm. Optimizes the policy \n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: My self created environment, specified above.\n",
    "        policy_estimator: Policy Function to be optimized \n",
    "        value_estimator: Value function approximator, used as a critic\n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: Time-discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and\n",
    "        episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\",\n",
    "                                                       \"reward\", \"next_state\",\n",
    "                                                       \"done\"])\n",
    "    \n",
    "    funds_wealth_all_episodes = []\n",
    "    \n",
    "    learning_fund_stats = np.zeros((num_episodes, num_timesteps, 5))\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        # Reset everything\n",
    "        prices = []\n",
    "        funds_wealth = []\n",
    "\n",
    "        # Create our learning_fund\n",
    "        learning_fund = LearningFund()\n",
    "        \n",
    "        # Create the funds \n",
    "        number_of_funds = 10\n",
    "        funds = [DynamicFund((i+1)*5) for i in range(number_of_funds)]\n",
    "        \n",
    "        # Add our learning fund\n",
    "        funds.append(learning_fund)\n",
    "\n",
    "        # Reset the environment \n",
    "        env.reset() \n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in range(num_timesteps):\n",
    "            \n",
    "            # get the demand (which is our action) of the learning fund\n",
    "            demand = learning_fund.get_demand(env.p_t) \n",
    "            \n",
    "            state = learning_fund.get_state(env.p_t)\n",
    "            \n",
    "            # Simulate a step in the environment,\n",
    "            # record the wealth of all funds in current_wealth\n",
    "            current_wealth = env.step(funds)\n",
    "            \n",
    "            # record the wealth of all funds and the current price\n",
    "            funds_wealth.append(current_wealth)\n",
    "            prices.append(env.p_t)\n",
    "            \n",
    "            # we assume one learning fund for the moment\n",
    "            next_state = learning_fund.get_state(env.p_t) \n",
    "\n",
    "            reward = learning_fund.performance\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(state=state, action=demand,\n",
    "                                      reward=reward, next_state=next_state,\n",
    "                                      done=env.done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Calculate TD Target\n",
    "            value_next = value_estimator.predict(next_state)\n",
    "            td_target = reward + discount_factor * value_next\n",
    "            td_error = td_target - value_estimator.predict(state)\n",
    "            \n",
    "            # Update the value estimator\n",
    "            value_estimator.update(state, td_target)\n",
    "            \n",
    "            # Update the policy estimator\n",
    "            # using the td error as our advantage estimate\n",
    "            policy_estimator.update(state, td_error, learning_fund.leverage)\n",
    "            \n",
    "            learning_fund_stats[i_episode][t] = np.array([env.p_t,\n",
    "                                                          demand,\n",
    "                                                          learning_fund.get_wealth(env.p_t),\n",
    "                                                          learning_fund.cash,\n",
    "                                                          learning_fund.shares])\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rt: {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes,\n",
    "                    stats.episode_rewards[i_episode - 1]), end=\"\")\n",
    "            \n",
    "                        \n",
    "            # env.done is True if one fund increases its wealth 50-fold\n",
    "            #if env.done:\n",
    "            #    break\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "        # After each episode, record the wealth of all funds\n",
    "        funds_wealth_all_episodes.append(funds_wealth)\n",
    "    \n",
    "        # Save the variables to disk.\n",
    "        checkpoint = \"./checkpoints/{}-ep{}\".format(experiment_name,i_episode)\n",
    "        save_path = saver.save(sess,checkpoint)         \n",
    "        print(\"\\nModel saved in path: {}\\n\".format(save_path))\n",
    "    \n",
    "    return stats, funds_wealth_all_episodes, learning_fund_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model from checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"./checkpoints/smart_2-ep29\"\n",
       "all_model_checkpoint_paths: \"./checkpoints/smart_2-ep29\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at saved checkpoints\n",
    "tf.train.get_checkpoint_state(\"./checkpoints/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/smart_2-ep29\n",
      "t: 2021 @ Episode 1/1 (-0.24768473207950592))"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-381d20cc07cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     stats_continued, funds_wealth_all_episodes_continued = actor_critic(env, policy_estimator, value_estimator,\n\u001b[0;32m---> 25\u001b[0;31m                                                                       num_episodes=num_episodes, discount_factor=0.95)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDuration: {} min\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-358fbd32db59>\u001b[0m in \u001b[0;36mactor_critic\u001b[0;34m(env, policy_estimator, value_estimator, num_episodes, num_timesteps, discount_factor)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;31m# Update the value estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mvalue_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtd_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0;31m# Update the policy estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-358fbd32db59>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, state, target, sess)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturize_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Simon/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# specify which checkpoint to load\n",
    "experiment_name = \"smart_2-ep29\"\n",
    "\n",
    "num_episodes = 1\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the environment\n",
    "env = Env()\n",
    "\n",
    "# first initialization of our learning fund\n",
    "learning_fund = LearningFund()\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "checkpoint = \"./checkpoints/{}\".format(experiment_name)\n",
    "\n",
    "experiment_name += \"_continued\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, checkpoint)\n",
    "\n",
    "    stats_continued, funds_wealth_all_episodes_continued = actor_critic(env, policy_estimator, value_estimator,\n",
    "                                                                      num_episodes=num_episodes, discount_factor=0.95)\n",
    "    \n",
    "print(\"\\nDuration: {} min\".format((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(funds_wealth_all_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
